View(comparar)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.3, gamma=0, max_depth=12, min_child_weight=1, subsample=1, colsample_bytree=1)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "error")
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.3, gamma=0, max_depth=12, min_child_weight=1, subsample=1, colsample_bytree=1)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "RMSE")
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.3, gamma=0, max_depth=12, min_child_weight=1, subsample=1, colsample_bytree=1)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.3, gamma=0, max_depth=12, min_child_weight=1, subsample=1, colsample_bytree=1)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 79,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
View(mat)
View(comparar)
rmse = 1/(7336)*sum((test$N - xgbpred)^2)
length(nrow(test))
nrow(test)
rmse <- 1/nrow(test)*sum((xgbpred - test$N)^2)
rm(list = ls())
sqrt(0.05)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
View(comparar)
rmse <- 1/nrow(test)*sum((xgbpred - test$N)^2)
rm(list = ls())
library(data.table); library(xgboost); library(tidyr); library(plyr)
load("../Accidentes de tráfico - Madrid/Cleaned_data/GeoAccidentalidad.RData")
load("../Accidentes de tráfico - Madrid/Cleaned_data/meteo_parcial.RData")
# 1_ En primer lugar vamos a probar solo los años 2013 y 2014, con meteo y accidentes únicamente
# (por no introducir variables espaciales todavía - intensidades de tráfico).
# Aunque utilizaremos los distritos también
# Nos aseguramos de coger solo accidentes individuales, y no tantos como personas implicadas.
acc <- GeoAccidentalidad[, .N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO, lat, lon)]
acc <- acc[,c("N", "lat", "lon") := NULL]
# También de que ambos tengan el mismo formato en las columnas que juntar: fecha y hora
acc$`RANGO HORARIO` <- substr(acc$`RANGO HORARIO`, 1, 5)
acc$`RANGO HORARIO` <- extract_numeric(acc$`RANGO HORARIO`)
acc[, FECHA := dmy(FECHA)]
meteo_parcial[, FECHA := dmy(FECHA)]
acc <- acc[year(FECHA) == 2013 | year(FECHA) == 2014]
# acc <- join(acc[year(FECHA) == 2013 | year(FECHA) == 2014], meteo_parcial)
# 2_ Ahora hay que obtener para cada día de la semana, hora, distrito y condiciones meteorológicas el número de accidentes. Esto es
# un acercamiento distinto al de tener una serie de puntos temporales como pasaba en la STNN. Antes hay que crear otro conjunto
# que nos servirá en nuestro proposito de asignar un número de accidentes a cada una de estas posibilidades: todas las posibles
# combinaciones de días de la semena, distritos y horas.
num_distritos <- 21
num_horas <- 24*num_distritos
dias_horas_zonas <- data.table(FECHA = sort(rep(seq(ymd('2013-01-01'), ymd('2014-12-31'), by = '1 day'), num_horas)),
"RANGO HORARIO" = sort(rep(seq(0,23), num_distritos)),
DISTRITO = unique(acc$DISTRITO))
dias_horas_zonas$`DIA SEMANA` <- toupper(weekdays(dias_horas_zonas$FECHA))
# 3_ Ahora si que podemos crear el dataset sobre el que hacer la regresión.
# Primero obtenemos el número de accidentes que ha habido en función de las variables relevantes,
# para a continuación hacer el merge con todos los rangos posibles y asignar a 0 todas aquellas
# posibilidades para las cuales no hubo accidente.
num_acc_sm <- acc[,.N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO)]
num_acc_sm <- join(dias_horas_zonas, num_acc_sm)
num_acc_sm[is.na(num_acc_sm)] <- 0
num_acc <- join(num_acc_sm, meteo_parcial)
num_acc <- na.omit(num_acc)
# 4_ Ya se está preparado para utilizar el xgboost
library(xgboost)
# La fecha ya no nos interesa
num_acc[, FECHA := NULL]
# Dividimos el dataset en dos: uno para el entrenamiento, otro para la validación.
sample <- seq(1, floor(.98*nrow(num_acc)))
train <- num_acc[sample, ]
test  <- num_acc[-sample, ]
svmfit = svm(N ~ ., data = train, type = "eps-regression", kernel = "sigmoid", cost = 10, scale = FALSE)
library("e1071", lib.loc="/usr/local/lib/R/site-library")
svmfit = svm(N ~ ., data = train, type = "eps-regression", kernel = "sigmoid", cost = 10, scale = FALSE)
svmpred <- predict.svm()
svmpred <- predict(svmfit, newdata = test)
library(ModelMetrics)
rmse(test$N, svmpred)
comparar <- data.table(test$N, svmpred)
View(comparar)
min(svmpred)
max(svmpred)
rm(list = ls())
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(num_acc)
num_acc <- num_acc[, 1:4]
View(num_acc)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(comparar)
View(comparar)
View(mat)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
sample <- seq(1, floor(.98*nrow(num_acc)))
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
View(num_acc_sm)
library(data.table); library(xgboost); library(tidyr); library(plyr)
load("../Accidentes de tráfico - Madrid/Cleaned_data/GeoAccidentalidad.RData")
load("../Accidentes de tráfico - Madrid/Cleaned_data/meteo_parcial.RData")
# 1_ En primer lugar vamos a probar solo los años 2013 y 2014, con meteo y accidentes únicamente
# (por no introducir variables espaciales todavía - intensidades de tráfico).
# Aunque utilizaremos los distritos también
# Nos aseguramos de coger solo accidentes individuales, y no tantos como personas implicadas.
acc <- GeoAccidentalidad[, .N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO, lat, lon)]
acc <- acc[,c("N", "lat", "lon") := NULL]
# También de que ambos tengan el mismo formato en las columnas que juntar: fecha y hora
acc$`RANGO HORARIO` <- substr(acc$`RANGO HORARIO`, 1, 5)
acc$`RANGO HORARIO` <- extract_numeric(acc$`RANGO HORARIO`)
acc[, FECHA := dmy(FECHA)]
meteo_parcial[, FECHA := dmy(FECHA)]
acc <- acc[year(FECHA) == 2013 | year(FECHA) == 2014]
# acc <- join(acc[year(FECHA) == 2013 | year(FECHA) == 2014], meteo_parcial)
# 2_ Ahora hay que obtener para cada día de la semana, hora, distrito y condiciones meteorológicas el número de accidentes. Esto es
# un acercamiento distinto al de tener una serie de puntos temporales como pasaba en la STNN. Antes hay que crear otro conjunto
# que nos servirá en nuestro proposito de asignar un número de accidentes a cada una de estas posibilidades: todas las posibles
# combinaciones de días de la semena, distritos y horas.
num_distritos <- 21
num_horas <- 24*num_distritos
dias_horas_zonas <- data.table(FECHA = sort(rep(seq(ymd('2013-01-01'), ymd('2014-12-31'), by = '1 day'), num_horas)),
"RANGO HORARIO" = sort(rep(seq(0,23), num_distritos)),
DISTRITO = unique(acc$DISTRITO))
dias_horas_zonas$`DIA SEMANA` <- toupper(weekdays(dias_horas_zonas$FECHA))
# 3_ Ahora si que podemos crear el dataset sobre el que hacer la regresión.
# Primero obtenemos el número de accidentes que ha habido en función de las variables relevantes,
# para a continuación hacer el merge con todos los rangos posibles y asignar a 0 todas aquellas
# posibilidades para las cuales no hubo accidente.
num_acc_sm <- acc[,.N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO)]
num_acc_sm <- join(dias_horas_zonas, num_acc_sm)
num_acc_sm[is.na(num_acc_sm)] <- 0
num_acc <- join(num_acc_sm, meteo_parcial)
num_acc <- na.omit(num_acc)
# 4_ Ya se está preparado para utilizar el xgboost
library(xgboost)
# La fecha ya no nos interesa
num_acc[, FECHA := NULL]
View(num_acc)
num_acc <- num_acc[, -c(5,6,8,9,10)]
View(num_acc)
sample <- seq(1, floor(.98*nrow(num_acc)))
train <- num_acc[sample, ]
test  <- num_acc[-sample, ]
# Ahora hay que pasar a numérico las variables que no lo son. Así, para las que son strings de esta manera
# puedes crear una variable dummy, es decir, estas nuevas matrices tienen tantas columnas como variables y posibles
# valores de las variables que no son numéricas, de forma que en cada fila se asigna 0 o 1 en cada columna según pertenezca
# a esa clase o no.
labels <- train$N
ts_label <- test$N
# new_tr <- model.matrix(~.+0,data = train[,-c("N"),with=F])
# new_ts <- model.matrix(~.+0,data = test[,-c("N"),with=F])
# Otra posibilidad es:
train$DISTRITO = as.numeric(factor(train$`DISTRITO`,unique(train$`DISTRITO`)))
train$`DIA SEMANA` = as.numeric(factor(train$`DIA SEMANA`,unique(train$`DIA SEMANA`)))
test$DISTRITO = as.numeric(factor(test$`DISTRITO`,unique(test$`DISTRITO`)))
test$`DIA SEMANA` = as.numeric(factor(test$`DIA SEMANA`,unique(test$`DIA SEMANA`)))
new_tr <- model.matrix(~.+0, data = train[,-c("N"), with=F])
new_ts <- model.matrix(~.+0, data = test[,-c("N"), with=F])
# Creamos las matrices con las que el xgb es capaz de trabajar
dtrain <- xgb.DMatrix(data = new_tr , label=labels)
dtest <- xgb.DMatrix(data = new_ts , label=ts_label)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.05, gamma=0, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
# library(ModelMetrics)
# No entiendo por que esto me da otras predicciones. En cualquier caso, las funciones van bien
# si utilizo xgbpred
# y_hat_xgb <- predict(xgb1, data.matrix(test))
# xgb_mae <- mae(test$N, y_hat_xgb)
# xgb_rmse <- rmse(test$N, y_hat_xgb)
# xgb_recall <- recall(test$N, y_hat_xgb)
View(mat)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=0.5, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=1, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=1.5, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.5, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=3, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.8, lambda = 1, max_depth=10, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.75, lambda = 1, max_depth=12, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.8, lambda = 1, max_depth=12, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 100,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.02, gamma=2.7, lambda = 1, max_depth=12, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.1, gamma=2.7, lambda = 1, max_depth=12, min_child_weight=5, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.7, lambda = 1, max_depth=12, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
View(comparar)
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
View(mat)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=0, lambda = 1, max_depth=12, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
View(mat)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=10, lambda = 1, max_depth=12, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
View(mat)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.7, lambda = 1, max_depth=12, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.7, lambda = 1, max_depth=15, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
View(mat)
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=1, lambda = 1, max_depth=12, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
View(mat)
rm(list = ls())
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
rmse(test$N, xgbpred)
rmse(test$N[1:1000], xgbpred[1:1000])
rmse(test$N[1:504], xgbpred[1:504])
rmse(test$N[505:1008], xgbpred[505:1008])
rmse(test$N[1009:1514], xgbpred[1009:1514])
rmse(test$N[1515:2019], xgbpred[1515:2019])
sum(test$N[1515:2019])
sum(test$N[1009:1514])
sum(test$N[505:1008])
sum(test$N[1:504])
save(mat, file = "Raw_data/mat.RData")
library(data.table); library(xgboost); library(tidyr); library(plyr)
load("../Accidentes de tráfico - Madrid/Cleaned_data/GeoAccidentalidad.RData")
load("../Accidentes de tráfico - Madrid/Cleaned_data/meteo_parcial.RData")
# 1_ En primer lugar vamos a probar solo los años 2013 y 2014, con meteo y accidentes únicamente
# (por no introducir variables espaciales todavía - intensidades de tráfico).
# Aunque utilizaremos los distritos también
# Nos aseguramos de coger solo accidentes individuales, y no tantos como personas implicadas.
acc <- GeoAccidentalidad[, .N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO, lat, lon)]
acc <- acc[,c("N", "lat", "lon") := NULL]
# También de que ambos tengan el mismo formato en las columnas que juntar: fecha y hora
acc$`RANGO HORARIO` <- substr(acc$`RANGO HORARIO`, 1, 5)
acc$`RANGO HORARIO` <- extract_numeric(acc$`RANGO HORARIO`)
acc[, FECHA := dmy(FECHA)]
meteo_parcial[, FECHA := dmy(FECHA)]
acc <- acc[year(FECHA) == 2013 | year(FECHA) == 2014]
# acc <- join(acc[year(FECHA) == 2013 | year(FECHA) == 2014], meteo_parcial)
# 2_ Ahora hay que obtener para cada día de la semana, hora, distrito y condiciones meteorológicas el número de accidentes. Esto es
# un acercamiento distinto al de tener una serie de puntos temporales como pasaba en la STNN. Antes hay que crear otro conjunto
# que nos servirá en nuestro proposito de asignar un número de accidentes a cada una de estas posibilidades: todas las posibles
# combinaciones de días de la semena, distritos y horas.
num_distritos <- 21
num_horas <- 24*num_distritos
dias_horas_zonas <- data.table(FECHA = sort(rep(seq(ymd('2013-01-01'), ymd('2014-12-31'), by = '1 day'), num_horas)),
"RANGO HORARIO" = sort(rep(seq(0,23), num_distritos)),
DISTRITO = unique(acc$DISTRITO))
dias_horas_zonas$`DIA SEMANA` <- toupper(weekdays(dias_horas_zonas$FECHA))
# 3_ Ahora si que podemos crear el dataset sobre el que hacer la regresión.
# Primero obtenemos el número de accidentes que ha habido en función de las variables relevantes,
# para a continuación hacer el merge con todos los rangos posibles y asignar a 0 todas aquellas
# posibilidades para las cuales no hubo accidente.
num_acc_sm <- acc[,.N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO)]
num_acc_sm <- join(dias_horas_zonas, num_acc_sm)
num_acc_sm[is.na(num_acc_sm)] <- 0
num_acc <- join(num_acc_sm, meteo_parcial)
num_acc <- na.omit(num_acc)
# 4_ Ya se está preparado para utilizar el xgboost
library(xgboost)
# La fecha ya no nos interesa
num_acc[, FECHA := NULL]
# num_acc <- num_acc[, -c(5,6,8,9,10)]
# Dividimos el dataset en dos: uno para el entrenamiento, otro para la validación.
sample <- seq(1, floor(.98*nrow(num_acc)))
train <- num_acc[sample, ]
test  <- num_acc[-sample, ]
# Ahora hay que pasar a numérico las variables que no lo son. Así, para las que son strings de esta manera
# puedes crear una variable dummy, es decir, estas nuevas matrices tienen tantas columnas como variables y posibles
# valores de las variables que no son numéricas, de forma que en cada fila se asigna 0 o 1 en cada columna según pertenezca
# a esa clase o no. ONE-HOT ENCODING
labels <- train$N
ts_label <- test$N
new_tr <- model.matrix(~.+0,data = train[,-c("N"),with=F])
new_ts <- model.matrix(~.+0,data = test[,-c("N"),with=F])
# Otra posibilidad es INTEGER ENCODING:
# train$DISTRITO = as.numeric(factor(train$`DISTRITO`,unique(train$`DISTRITO`)))
# train$`DIA SEMANA` = as.numeric(factor(train$`DIA SEMANA`,unique(train$`DIA SEMANA`)))
#
# test$DISTRITO = as.numeric(factor(test$`DISTRITO`,unique(test$`DISTRITO`)))
# test$`DIA SEMANA` = as.numeric(factor(test$`DIA SEMANA`,unique(test$`DIA SEMANA`)))
new_tr <- model.matrix(~.+0, data = train[,-c("N"), with=F])
new_ts <- model.matrix(~.+0, data = test[,-c("N"), with=F])
# Creamos las matrices con las que el xgb es capaz de trabajar
dtrain <- xgb.DMatrix(data = new_tr , label=labels)
dtest <- xgb.DMatrix(data = new_ts , label=ts_label)
View(train)
new_ts
params <- list(booster = "gbtree", objective = "reg:linear",
eta=0.08, gamma=2.7, lambda = 1, max_depth=12, min_child_weight=6, subsample=0.7, colsample_bytree=0.9)
# xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 100, nfold = 5,
#                  showsd = T, stratified = T, print_every_n = 10, early_stop_round = 20, maximize = F)
xgb1 <- xgb.train (params = params, data = dtrain, nrounds = 70,
watchlist = list(val=dtest,train=dtrain), verbose = 1, print_every_n = 10, early_stop_round = 10,
maximize = F , eval_metric = "rmse") # Aunque esto del error lo hace automático al poner regresión creo
xgbpred <- predict (xgb1,dtest)
comparar <- data.table(test$N, xgbpred)
# rmse <- sqrt(1/nrow(test)*sum((xgbpred - test$N)^2))
mat <- xgb.importance (feature_names = colnames(new_tr),model = xgb1)
xgb.plot.importance (importance_matrix = mat[1:10])
View(comparar)
sum(test$N)
sum(xgbpred)
as.data.table(xgbpred)
xgbpred <- as.data.table(xgbpred)
sum(xgbpred[xgbpred >= 0.1])
View(mat)
summary(test)
head(test)
ls(test)
View(test)
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
View(mat)
mat2 <- mat
save(mat2, file = "Raw_data/mat2.RData")
rm(list = ls())
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
rm(list = ls())
source('~/Accidentes de tráfico - Madrid/Scripts/Fase 2/prueba_xgboost.R')
list(rmse(test$N, y_hat_xgb))
list(rmse(test$N, xgbpred))
RMSE <- c(rmse(test$N, xgbpred[1:n]), rmse(test$N, xgbpred[n+1:2*n]), rmse(test$N, xgbpred[2*n+1:3*n]), rmse(test$N, xgbpred[3*n+1:4*n]), rmse(test$N, xgbpred[4*n+1:5*n]))
n = 504
RMSE <- c(rmse(test$N, xgbpred[1:n]), rmse(test$N, xgbpred[n+1:2*n]), rmse(test$N, xgbpred[2*n+1:3*n]), rmse(test$N, xgbpred[3*n+1:4*n]), rmse(test$N, xgbpred[4*n+1:5*n]))
RMSE
RMSE <- c(rmse(test$N, xgbpred[1:n]), rmse(test$N, xgbpred[n+1:2*n]), rmse(test$N, xgbpred[2*n+1:3*n]), rmse(test$N, xgbpred[3*n+1:4*n]), rmse(test$N, xgbpred[(4*n+1):5*n]))
RMSE
RMSE <- c(rmse(test$N[1:n], xgbpred[1:n]), rmse(test$N, xgbpred[n+1:2*n]), rmse(test$N, xgbpred[2*n+1:3*n]), rmse(test$N, xgbpred[3*n+1:4*n]), rmse(test$N[(4*n+1):5*n], xgbpred[(4*n+1):5*n]))
RMSE
RMSE <- c(rmse(test$N[1:n], xgbpred[1:n]), rmse(test$N[n+1:2*n], xgbpred[n+1:2*n]), rmse(test$N, xgbpred[2*n+1:3*n]), rmse(test$N, xgbpred[3*n+1:4*n]), rmse(test$N[(4*n+1):5*n], xgbpred[(4*n+1):5*n]))
RMSE
RMSE <- c(rmse(test$N[1:n], xgbpred[1:n]), rmse(test$N[(n+1):(2*n)], xgbpred[(n+1):(2*n)]), rmse(test$N, xgbpred[2*n+1:3*n]), rmse(test$N, xgbpred[3*n+1:4*n]), rmse(test$N[(4*n+1):5*n], xgbpred[(4*n+1):5*n]))
RMSE
RMSE <- c(rmse(test$N[1:n], xgbpred[1:n]), rmse(test$N[(n+1):(2*n)], xgbpred[(n+1):(2*n)]), rmse(test$N[(2*n+1):3*n], xgbpred[(2*n+1):3*n]), rmse(test$N[(3*n+1):4*n], xgbpred[(3*n+1):4*n]), rmse(test$N[(4*n+1):5*n], xgbpred[(4*n+1):5*n]))
RMSE
RMSE <- c(rmse(test$N[1:n], xgbpred[1:n]), rmse(test$N[(n+1):(2*n)], xgbpred[(n+1):(2*n)]), rmse(test$N[(2*n+1):(3*n)], xgbpred[(2*n+1):(3*n)]), rmse(test$N[(3*n+1):4*n], xgbpred[(3*n+1):4*n]), rmse(test$N[(4*n+1):5*n], xgbpred[(4*n+1):5*n]))
RMSE
RMSE <- c(rmse(test$N[1:n], xgbpred[1:n]), rmse(test$N[(n+1):(2*n)], xgbpred[(n+1):(2*n)]), rmse(test$N[(2*n+1):(3*n)], xgbpred[(2*n+1):(3*n)]), rmse(test$N[(3*n+1):(4*n)], xgbpred[(3*n+1):(4*n)]), rmse(test$N[(4*n+1):(5*n)], xgbpred[(4*n+1):(5*n)]))
n_acc_dia <- c(sum(test$N[1:n]), sum(test$N[(n+1):(2*n)]), sum(test$N[(2*n+1):(3*n)]),sum(test$N[(3*n+1):(4*n)]), sum(test$N[(4*n+1):(5*n)]))
RMSE
rmse_xgboost_1 <- rbind(RMSE, n_acc_dia)
View(rmse_xgboost_1)
colnames(rmse_xgboost_1) <- c("Día 1", "Día 2", "Día 3", "Día 4", "Día 5")
View(rmse_xgboost_1)
rownames(rmse_xgboost_1)[2] <- "Número de accidentes por día"
View(rmse_xgboost_1)
rownames(rmse_xgboost_1)[2] <- "Número de accidentes\n por día"
View(rmse_xgboost_1)
rownames(rmse_xgboost_1)[2] <- "Número de accs.\n por día"
View(rmse_xgboost_1)
rownames(rmse_xgboost_1)[2] <- "Número de accidentes por día"
rmse_xgboost_1[1]
rmse_xgboost_1[,1]
rmse_xgboost_1[:,]
rmse_xgboost_1[,1]
rmse_xgboost_1[1,]
rmse_xgboost_1[2,] <- round(rmse_xgboost_1[2,], 0)
View(rmse_xgboost_1)
rmse_xgboost_1[2,1] <- round(rmse_xgboost_1[2,1], 0)
View(rmse_xgboost_1)
rmse_xgboost_1[2,1] <- round(rmse_xgboost_1[2,1], 1)
View(rmse_xgboost_1)
rmse_xgboost_1[2,] <- round(rmse_xgboost_1[2,], 0)
save(rmse_xgboost_1, file = "\Raw_data/rmse_xgboost_1.RData")
save(rmse_xgboost_1, file = "Raw_data/rmse_xgboost_1.RData")
load("~/Accidentes de tráfico - Madrid/Cleaned_data/GeoAccidentalidad.RData")
acc <- GeoAccidentalidad[, .N, by = list(FECHA, `RANGO HORARIO`, `DIA SEMANA`, DISTRITO, lat, lon)]
acc <- acc[,c("N", "lat", "lon") := NULL]
View(acc)
acc[,.N, by = DISTRITO]
rm(list = ls())
